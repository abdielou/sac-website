---
phase: 21-content-migration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/migrate-blog-to-s3.js
  - .env.template
autonomous: true

must_haves:
  truths:
    - "Running the script with --dry-run previews all 77 posts and 215 images that would be migrated, without uploading anything"
    - "Running the script with --live uploads all images to S3_IMAGES_BUCKET_NAME and all articles to S3_ARTICLES_BUCKET_NAME"
    - "Image references in migrated article content point to S3 URLs instead of /static/images/blog/ paths"
    - "The articles/index.json in S3 contains index entries for all 77 migrated articles sorted newest-first"
    - "A detailed migration log file is created with every action, timestamp, and status"
  artifacts:
    - path: "scripts/migrate-blog-to-s3.js"
      provides: "Complete migration script with dry-run and live modes"
      contains: "gray-matter"
    - path: ".env.template"
      provides: "S3_IMAGES_BUCKET_NAME environment variable documentation"
      contains: "S3_IMAGES_BUCKET_NAME"
  key_links:
    - from: "scripts/migrate-blog-to-s3.js"
      to: "lib/articles-s3.js"
      via: "putArticleJSON and putArticleIndex for article storage"
      pattern: "putArticleJSON|putArticleIndex"
    - from: "scripts/migrate-blog-to-s3.js"
      to: "data/blog/**/*.mdx"
      via: "gray-matter parsing of MDX frontmatter and content"
      pattern: "gray-matter"
    - from: "scripts/migrate-blog-to-s3.js"
      to: "public/static/images/blog/**/*"
      via: "fs.readFileSync for image binary data upload to S3"
      pattern: "readFileSync"
---

<objective>
Build the complete migration script that reads all existing MDX blog posts and images from the filesystem, transforms them to JSON format, rewrites image references to S3 URLs, and uploads everything to S3.

Purpose: This is the core tooling for Phase 21. The script handles both dry-run preview and live migration, following a two-phase approach (images first, then articles) with fail-fast error handling.

Output: `scripts/migrate-blog-to-s3.js` migration script and updated `.env.template` with S3_IMAGES_BUCKET_NAME.
</objective>

<execution_context>
@C:/Users/abdie/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/abdie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-content-migration/21-RESEARCH.md
@.planning/phases/21-content-migration/21-CONTEXT.md
@.planning/phases/19-s3-article-data-layer/19-01-SUMMARY.md
@lib/articles-s3.js
@lib/articles.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create complete migration script</name>
  <files>scripts/migrate-blog-to-s3.js</files>
  <action>
Create `scripts/migrate-blog-to-s3.js` as a Node.js CommonJS script. This is the core migration script. It MUST implement everything below in a single file.

**CLI Interface:**
- `node scripts/migrate-blog-to-s3.js --dry-run` — preview all changes without uploading
- `node scripts/migrate-blog-to-s3.js --live` — execute the actual migration
- No flag = error with usage instructions
- Validate required env vars before starting: `S3_ARTICLES_BUCKET_NAME`, `S3_IMAGES_BUCKET_NAME`, `AWS_REGION`, `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` (only for --live mode; dry-run skips S3 env validation)

**Constants:**
- `BLOG_ROOT`: `path.resolve(__dirname, '..', 'data', 'blog')`
- `IMAGES_ROOT`: `path.resolve(__dirname, '..', 'public', 'static', 'images', 'blog')`
- `REQUIRED_FRONTMATTER`: `['title', 'date', 'tags', 'draft', 'summary']`
- `MIME_TYPES`: `{ '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.png': 'image/png', '.gif': 'image/gif', '.webp': 'image/webp', '.jfif': 'image/jpeg' }`
- `CONCURRENCY_LIMIT`: 10

**Phase 0 — Discovery:**
1. Recursively find all `.mdx` and `.md` files in `data/blog/` (use `fs.readdirSync` recursive traversal)
2. Recursively find all image files in `public/static/images/blog/` (match extensions: .jpg, .jpeg, .png, .gif, .webp, .jfif)
3. Log counts: "Found N posts, M images"

**Phase 1 — Image Upload (images first, BEFORE articles):**
1. Collect all images from `public/static/images/blog/` into a list of `{ localPath, s3Key }` pairs
   - s3Key format: `images/blog/YYYY/MM/filename.ext` (mirror filesystem structure under `public/static/`)
   - Example: `public/static/images/blog/2022/07/photo.jpg` -> s3Key = `images/blog/2022/07/photo.jpg`
2. Upload images with parallel concurrency of 10 using a promise pool pattern (no external library)
3. Use `aws-sdk` v2 `S3.putObject()` directly (NOT the articles-s3.js client which targets a different bucket)
4. Create a NEW S3 client for images: `new AWS.S3({ region: process.env.AWS_REGION })` (no custom endpoint — images go to the real S3 images bucket, not localstack)
5. Set `ContentType` from MIME_TYPES map based on file extension
6. **Verify each upload**: After putObject resolves, consider it successful (aws-sdk throws on failure). Log each: `[42/215] Uploaded images/blog/2022/07/photo.jpg`
7. **Fail fast**: If ANY image upload fails, stop immediately and report which image failed
8. In dry-run mode: log what WOULD be uploaded but skip actual S3 calls

**Phase 2 — Article Processing and Upload:**
1. For each MDX/MD file:
   a. Parse with `gray-matter` (already installed): `const { data: frontmatter, content } = matter(source)`
   b. **Derive slug from filesystem path** (CRITICAL — do NOT use generateSlug from lib/articles.js):
      ```javascript
      const relative = path.relative(BLOG_ROOT, filePath).replace(/\\/g, '/')
      const slug = relative.replace(/\.(mdx|md)$/, '')
      ```
      Example: `data/blog/2017/04/13/PlanetaDosSoles04132017.mdx` -> slug = `2017/04/13/PlanetaDosSoles04132017`
   c. **Validate required frontmatter**: Check `title`, `date`, `tags`, `draft`, `summary` all exist. If ANY is missing, stop migration and report which file and which field is missing. EXCEPTION: `examples/` posts may have empty summary — allow empty string but not undefined.
   d. **Rewrite image references in content** — handle ALL FOUR patterns:
      - Pattern 1: `src={'/static/images/blog/...'}` (JSX expression, single quotes) -> `src="S3_URL"`
      - Pattern 2: `src="/static/images/blog/..."` (JSX attribute, double quotes) -> `src="S3_URL"`
      - Pattern 3: `src={"/static/images/blog/..."}` (JSX expression, double quotes) -> `src="S3_URL"`
      - Pattern 4: `![alt](/static/images/blog/...)` (Markdown image) -> `![alt](S3_URL)`
      S3 URL format: `https://{S3_IMAGES_BUCKET_NAME}.s3.amazonaws.com/{s3Key}`
      **URL encoding**: For image paths with spaces, each path segment must be URI-encoded. Write a helper `encodeS3Url(bucketName, s3Key)` that splits on `/`, encodes each segment with `encodeURIComponent`, and joins with `/`.
   e. **Rewrite frontmatter `images` field**: For each entry in `frontmatter.images` (if array):
      - If starts with `static/images/blog/` or `/static/images/blog/` -> replace with S3 URL
      - If starts with something else (e.g., `static/images/telescope_color.png`) -> leave unchanged (non-blog image)
      - If empty string or falsy -> leave as-is
   f. **Missing image check**: For every image reference found in content that matches `/static/images/blog/*`, verify the corresponding file exists in `public/static/images/blog/`. If not found, stop migration and report which article references which missing image. Non-blog image references (e.g., `/static/images/ocean.jpg`, `/static/images/canada/*`) should be LEFT UNCHANGED — they are not blog images and won't be migrated.
   g. **Build article JSON object**:
      ```javascript
      {
        title: frontmatter.title,
        date: frontmatter.date instanceof Date ? frontmatter.date.toISOString() : String(frontmatter.date),
        lastmod: frontmatter.lastmod ? (frontmatter.lastmod instanceof Date ? frontmatter.lastmod.toISOString() : String(frontmatter.lastmod)) : null,
        tags: frontmatter.tags || [],
        summary: frontmatter.summary || '',
        content: rewrittenContent,  // MDX body with rewritten image refs
        images: rewrittenImages,    // frontmatter images with rewritten S3 URLs
        imgWidth: frontmatter.imgWidth || null,
        imgHeight: frontmatter.imgHeight || null,
        authors: frontmatter.authors || [],
        draft: frontmatter.draft !== undefined ? frontmatter.draft : false,
        archived: frontmatter.archived !== undefined ? frontmatter.archived : false,
        slug: slug,
      }
      ```
   h. **Upload article JSON**: Use `putArticleJSON()` from `lib/articles-s3.js` with key = `articles/${slug}.json`
      IMPORTANT: The migration script is CommonJS but lib/articles-s3.js uses ES modules. Since this is a Node.js script run directly (not via Next.js), we CANNOT import ES modules. Instead, create the S3 putObject call directly using aws-sdk (same pattern as articles-s3.js but inline in the script). Do NOT try to require/import lib/articles-s3.js.
   i. Log each: `[42/77] Migrated 2017/04/13/PlanetaDosSoles04132017`

2. **Fail fast**: If any article fails to parse, validate, or upload, stop immediately

**Phase 3 — Index Build:**
1. After ALL articles are uploaded, build the complete index from collected metadata
2. Use INDEX_FIELDS to extract only index fields from each article: `['title', 'date', 'lastmod', 'tags', 'summary', 'images', 'imgWidth', 'imgHeight', 'authors', 'draft', 'archived', 'slug']` — hardcode these in the script (don't import from lib/articles.js since it's ESM)
3. Sort articles newest-first by date
4. Build index object: `{ articles: [...], updatedAt: new Date().toISOString() }`
5. Upload to `articles/index.json` in S3_ARTICLES_BUCKET_NAME via S3 putObject
6. In dry-run mode: log summary of what index would contain

**Migration Log File:**
- Create log file at `scripts/migration-log-{TIMESTAMP}.jsonl` where TIMESTAMP is ISO format with dashes (e.g., `2026-02-13T10-30-00`)
- Each line is a JSON object: `{ timestamp, action, type, source, destination, status, error?, details? }`
- Actions: `image_upload`, `article_migrate`, `index_build`, `migration_start`, `migration_complete`
- Types: `image`, `article`, `index`, `system`
- In dry-run mode: still create log file but with `"mode": "dry-run"` in each entry and `status: "would_migrate"`

**Progress Reporting:**
- Show running counts: `[42/215] Uploaded images/blog/2022/07/photo.jpg`
- After each phase, show phase summary: `Phase 1 complete: 215/215 images uploaded`
- At end, show final summary:
  ```
  Migration Complete!
  - Posts migrated: 77
  - Images uploaded: 215
  - Index entries: 77
  - Duration: Xs
  - Log file: scripts/migration-log-TIMESTAMP.jsonl
  ```

**Dry-run mode specifics:**
- Output structured report:
  1. Posts to migrate (slug, title, draft status) — in table format
  2. Images to upload (count by year/folder)
  3. Image reference patterns detected per post (aggregate summary)
  4. Any missing image warnings that would cause fail-fast
  5. Summary counts
- Do NOT make any S3 calls
- DO still validate all frontmatter and check for missing images (catch issues before live run)

**Spot-check list** (output at the end of both dry-run and live):
```
Posts to manually spot-check:
1. Oldest: /blog/2017/04/13/PlanetaDosSoles04132017
2. Newest: /blog/2025/08/21/siete-cometas-tendran-un-leve-acercamiento-a-la-tierra
3. Video embeds: /blog/2024/11/26/Starship_Electrophonic_Sound
4. Complex layout: /blog/telescopios
5. Many images: /blog/2019/10/07/Saturn20
```
  </action>
  <verify>
Run `node scripts/migrate-blog-to-s3.js --dry-run` and confirm:
1. Script exits cleanly without S3 errors (dry-run should not need S3 credentials)
2. Output shows all 77 posts discovered
3. Output shows all 215 images discovered
4. No missing-image or missing-frontmatter errors
5. Log file created at scripts/migration-log-*.jsonl
6. Spot-check list displayed at the end
  </verify>
  <done>
The migration script runs in dry-run mode without errors, discovers all 77 posts and 215 images, validates all frontmatter fields, verifies all image references resolve to existing files, creates a JSONL log file, and outputs the spot-check list. The script is ready for live execution in Plan 21-02.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add S3_IMAGES_BUCKET_NAME to env template</name>
  <files>.env.template</files>
  <action>
Add the `S3_IMAGES_BUCKET_NAME` environment variable to `.env.template` in the S3 section (next to `S3_ARTICLES_BUCKET_NAME`).

Add this line after the `S3_ARTICLES_BUCKET_NAME=` line:
```
# S3 bucket for blog images (public, separate from articles)
S3_IMAGES_BUCKET_NAME=
```

This is a new env var required for the migration. The images bucket is separate from the articles bucket per Phase 19 decision (bucket separation for articles vs images).
  </action>
  <verify>
1. `grep S3_IMAGES_BUCKET_NAME .env.template` returns the new line
2. The variable appears in the S3 section near S3_ARTICLES_BUCKET_NAME
  </verify>
  <done>`.env.template` includes `S3_IMAGES_BUCKET_NAME` in the S3 configuration section.</done>
</task>

</tasks>

<verification>
1. `node scripts/migrate-blog-to-s3.js` (no args) shows usage error
2. `node scripts/migrate-blog-to-s3.js --dry-run` completes without errors, discovers 77 posts and 215 images
3. Dry-run validates all frontmatter (would fail-fast if any required field missing)
4. Dry-run verifies all image references resolve to existing files
5. Log file created at `scripts/migration-log-*.jsonl`
6. `.env.template` contains `S3_IMAGES_BUCKET_NAME`
</verification>

<success_criteria>
- Migration script exists at scripts/migrate-blog-to-s3.js and runs in dry-run mode without errors
- All 77 posts are discovered and parsed successfully
- All 215 images are discovered
- All image references in content resolve to existing filesystem files
- S3_IMAGES_BUCKET_NAME is documented in .env.template
- Log file is created during dry-run
- Spot-check list is displayed
</success_criteria>

<output>
After completion, create `.planning/phases/21-content-migration/21-01-SUMMARY.md`
</output>
