---
phase: 21-content-migration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/migrate-blog-to-s3.js
  - .env.template
autonomous: true

must_haves:
  truths:
    - "Migration script can parse all 77 MDX/MD blog posts extracting frontmatter and content"
    - "Migration script uploads all 215 blog images to S3 images bucket preserving path structure"
    - "Migration script rewrites all image references in article content to S3 URLs"
    - "Migration script creates article JSON files in S3 articles bucket with correct slug format"
    - "Migration script rebuilds articles/index.json with all migrated posts"
    - "Dry-run mode previews all operations without modifying S3"
    - "Script stops on first error (fail fast) for missing images, malformed frontmatter, or upload failures"
    - "Detailed migration log file is written with every post/image migrated and timestamps"
  artifacts:
    - path: "scripts/migrate-blog-to-s3.js"
      provides: "Complete migration script with --dry-run and --live modes"
      min_lines: 200
    - path: ".env.template"
      provides: "S3_IMAGES_BUCKET_NAME env var for public image bucket"
      contains: "S3_IMAGES_BUCKET_NAME"
  key_links:
    - from: "scripts/migrate-blog-to-s3.js"
      to: "aws-sdk S3"
      via: "direct S3 putObject for images and article JSON"
      pattern: "putObject.*Bucket"
    - from: "scripts/migrate-blog-to-s3.js"
      to: "data/blog/**/*.mdx"
      via: "glob + gray-matter frontmatter parsing"
      pattern: "gray-matter|matter\\("
    - from: "scripts/migrate-blog-to-s3.js"
      to: "articles/index.json"
      via: "putObject to write rebuilt index after all articles migrated"
      pattern: "articles/index\\.json"
---

<objective>
Build the complete content migration script that converts all existing MDX blog posts and images from the filesystem to S3, supporting both dry-run preview and live execution modes.

Purpose: This script is the core tool for Phase 21 — it parses all MDX posts in `data/blog/`, uploads all images from `public/static/images/blog/` to the S3 images bucket, rewrites image references in article content to point to S3 URLs, creates article JSON files in the S3 articles bucket, and rebuilds the article index. It must support `--dry-run` for safe preview and `--live` for actual migration.

Output: `scripts/migrate-blog-to-s3.js` migration script, updated `.env.template` with `S3_IMAGES_BUCKET_NAME`
</objective>

<execution_context>
@C:/Users/abdie/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/abdie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-content-migration/21-CONTEXT.md
@.planning/phases/19-s3-article-data-layer/19-01-SUMMARY.md
@lib/articles-s3.js
@lib/articles.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create migration script with MDX parsing, image upload, article creation, and index rebuild</name>
  <files>scripts/migrate-blog-to-s3.js, .env.template</files>
  <action>
Create `scripts/migrate-blog-to-s3.js` — a Node.js CLI script (no Next.js required) that migrates all blog content to S3. Use `gray-matter` for frontmatter parsing (already in package.json) and `aws-sdk` (already in package.json) for S3 operations. Use `glob` (already in package.json) for file discovery.

**CLI interface:**
- `node scripts/migrate-blog-to-s3.js --dry-run` — preview all operations, write log, no S3 changes
- `node scripts/migrate-blog-to-s3.js --live` — execute migration, write log, upload to S3
- No flag or invalid flag → print usage and exit

**Environment variables required:**
- `AWS_REGION` (existing)
- `AWS_ACCESS_KEY_ID` (existing)
- `AWS_SECRET_ACCESS_KEY` (existing)
- `S3_ARTICLES_BUCKET_NAME` (existing) — for article JSON files
- `S3_IMAGES_BUCKET_NAME` (new) — for public image files
- `AWS_S3_ENDPOINT` (optional, for localstack)
- Fail fast if any required env var is missing in --live mode

**Step 1 — Discover all posts:**
- Glob `data/blog/**/*.{mdx,md}` to find all posts
- Sort by file path for deterministic order
- Log total count

**Step 2 — Parse each post with gray-matter:**
- Extract frontmatter fields: title, date, lastmod, tags, draft, archived, summary, images, imgWidth, imgHeight, authors
- Validate required fields: title, date — if missing, stop with error showing file path and missing field(s)
- Extract content (everything after frontmatter)
- Generate slug using same logic as `lib/articles.js` `generateSlug(title, date)`:
  - YYYY/MM/DD/title-slug format
  - Spanish accent removal (copy ACCENT_MAP from lib/articles.js)
  - Special handling: for posts in `data/blog/examples/` or `data/blog/telescopios.mdx`, use the date from frontmatter (even if year 2000) to generate slug — same rules apply

**Step 3 — Collect all image references from each post:**
Auto-detect all image reference patterns in MDX content AND frontmatter. The patterns found in the codebase are:
1. Frontmatter `images:` field — paths like `static/images/blog/2025/08/21/file.jpg` (no leading slash)
2. JSX `src={'/static/images/blog/...'}` — curly brace string with leading slash
3. JSX `src="/static/images/blog/..."` — quoted string with leading slash
4. Some frontmatter `images:` have paths like `static/images/telescope_color.png` (outside blog dir)
5. Log ALL unique image reference patterns found across all posts at the start

For each image reference found:
- Normalize to a filesystem path: resolve relative to `public/` (e.g., `static/images/blog/X` → `public/static/images/blog/X`, `/static/images/blog/X` → `public/static/images/blog/X`)
- Verify the file exists on disk — if not, STOP migration immediately with error: "Missing image: {path} referenced in {post-file}"
- Track unique images to avoid duplicate uploads

**Step 4 — Upload images to S3 (--live mode only):**
- Upload to `S3_IMAGES_BUCKET_NAME` bucket
- S3 key: preserve path structure → `images/blog/YYYY/MM/filename.jpg` (strip `public/static/` prefix, keep rest)
- For images outside blog dir (like `static/images/telescope_color.png`), use key `images/telescope_color.png` (strip `static/` prefix)
- Set ContentType based on extension: `.jpg/.jpeg` → `image/jpeg`, `.png` → `image/png`, `.gif` → `image/gif`, `.webp` → `image/webp`, `.svg` → `image/svg+xml`
- Upload sequentially (simpler, fail-fast compatible — 215 images is manageable)
- Verify each upload succeeds before continuing
- Log each upload: `[IMAGE] Uploaded: {local-path} → s3://{bucket}/{key}`
- In dry-run mode: log what WOULD be uploaded without uploading

**Step 5 — Rewrite image references in content:**
For each post, replace ALL image references in content body:
- `/static/images/blog/...` → `https://{S3_IMAGES_BUCKET_NAME}.s3.amazonaws.com/images/blog/...`
- `static/images/blog/...` (no leading slash) → same S3 URL
- `/static/images/telescope_color.png` → `https://{S3_IMAGES_BUCKET_NAME}.s3.amazonaws.com/images/telescope_color.png`
Also rewrite the `images` array in frontmatter/article metadata the same way.

**Step 6 — Create article JSON and upload to S3 (--live mode only):**
For each post, create article object matching the schema in `lib/articles.js`:
```js
{
  title, date (ISO string), lastmod (ISO string), tags, summary, content (rewritten),
  images (rewritten S3 URLs), imgWidth, imgHeight, authors,
  draft (boolean, default false), archived (boolean, default false), slug
}
```
- S3 key: `articles/{slug}.json`
- Upload to `S3_ARTICLES_BUCKET_NAME` bucket
- ContentType: `application/json`
- Log each: `[ARTICLE] Created: articles/{slug}.json`
- In dry-run: log what WOULD be created

**Step 7 — Rebuild article index:**
After ALL articles uploaded (--live mode only):
- Build index with same structure as `lib/articles.js` INDEX_FIELDS:
  `{ articles: [{title, date, lastmod, tags, summary, images, imgWidth, imgHeight, authors, draft, archived, slug}, ...], updatedAt: ISO_STRING }`
- Sort articles by date descending
- Upload to `articles/index.json` in S3_ARTICLES_BUCKET_NAME
- Log: `[INDEX] Rebuilt articles/index.json with {N} articles`
- In dry-run: log what the index WOULD contain

**Step 8 — Write migration log:**
Write `migration-log.json` to project root with:
```js
{
  mode: "dry-run" | "live",
  startedAt: ISO_STRING,
  completedAt: ISO_STRING,
  posts: { total, migrated, drafts, archived, published },
  images: { total, uploaded },
  articles: [{ slug, title, file, draft, archived, imageCount }],
  spotCheckList: [
    { slug, reason: "oldest post" },
    { slug, reason: "newest post" },
    { slug, reason: "post with ResponsiveReactPlayer" },
    { slug, reason: "post with multiple images" },
    { slug, reason: "draft post" }
  ]
}
```

**Spot-check list selection:**
- Oldest post by date
- Newest post by date
- A post using `ResponsiveReactPlayer` component (e.g., Saturn20.mdx)
- A post with the most images in content
- A draft post (e.g., one from `data/blog/2024/07/10/`)

**Step 9 — Print summary:**
At the end, print a clear summary to console:
```
=== Migration Summary (DRY-RUN|LIVE) ===
Posts:      {N} total ({published} published, {drafts} drafts, {archived} archived)
Images:     {N} unique images {uploaded|to upload}
Articles:   {N} JSON files {created|to create}
Index:      {rebuilt|to rebuild} with {N} entries
Log:        migration-log.json

Spot-check these posts manually:
  1. /blog/{slug} — {reason}
  2. /blog/{slug} — {reason}
  ...
```

**Update .env.template:**
Add `S3_IMAGES_BUCKET_NAME=` entry after the existing `S3_ARTICLES_BUCKET_NAME=` line with a comment: `# S3 bucket for public blog images (separate from articles)`
  </action>
  <verify>
1. `node scripts/migrate-blog-to-s3.js` (no flag) prints usage message and exits
2. `node scripts/migrate-blog-to-s3.js --dry-run` completes without errors, prints summary showing 77 posts and 215 images, creates migration-log.json
3. Verify migration-log.json contains all posts with correct slugs and a spotCheckList
4. Verify .env.template contains S3_IMAGES_BUCKET_NAME
5. `npx eslint scripts/migrate-blog-to-s3.js` passes (or has only warnings, no errors)
  </verify>
  <done>
Migration script runs in dry-run mode, correctly parsing all 77 MDX/MD posts, detecting all 215 images, generating correct slugs, previewing S3 operations, and producing migration-log.json with spot-check list. The .env.template includes S3_IMAGES_BUCKET_NAME.
  </done>
</task>

</tasks>

<verification>
1. `node scripts/migrate-blog-to-s3.js --dry-run` completes with 0 exit code
2. migration-log.json exists and contains all 77 posts
3. Every post in migration-log.json has a valid YYYY/MM/DD/title-slug format slug
4. All image references in migration-log.json are accounted for
5. .env.template has S3_IMAGES_BUCKET_NAME entry
</verification>

<success_criteria>
- Migration script exists at scripts/migrate-blog-to-s3.js
- Dry-run mode works end-to-end without S3 credentials (no uploads, just parsing and logging)
- All 77 blog posts are parsed with correct frontmatter extraction
- All image references are detected and filesystem existence verified
- Slug generation matches lib/articles.js format (YYYY/MM/DD/title-slug)
- migration-log.json produced with complete results and spot-check list
- S3_IMAGES_BUCKET_NAME added to .env.template
</success_criteria>

<output>
After completion, create `.planning/phases/21-content-migration/21-01-SUMMARY.md`
</output>
