---
phase: 21-content-migration
plan: 02
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - scripts/migrate-blog-to-s3.js
autonomous: false

must_haves:
  truths:
    - "All 77 articles exist in S3 at articles/{slug}.json with correct frontmatter and content"
    - "All 215 images exist in S3 at images/blog/... in the images bucket"
    - "The articles/index.json contains 77 entries sorted newest-first"
    - "The public blog renders migrated posts correctly (verified by spot-checking oldest, newest, and complex posts)"
    - "Original MDX files and blog images are archived on a backup git branch"
  artifacts:
    - path: "articles/index.json (in S3)"
      provides: "Complete article index with all 77 migrated articles"
    - path: "scripts/migration-log-*.jsonl"
      provides: "Detailed log of every migration action with timestamps"
  key_links:
    - from: "S3 articles bucket"
      to: "app/blog/[...slug]/page.js"
      via: "getArticle(slug) reads article JSON from S3"
      pattern: "getArticle"
    - from: "S3 images bucket"
      to: "article content"
      via: "Image src attributes point to S3 image URLs"
      pattern: "s3\\.amazonaws\\.com/images/blog"
---

<objective>
Execute the live migration, verify all content landed correctly in S3, spot-check rendering, and archive the original files to a backup git branch.

Purpose: This is the execution and verification plan. Plan 21-01 built the tool; this plan uses it and validates the results.

Output: All blog content migrated to S3, verified, and originals archived.
</objective>

<execution_context>
@C:/Users/abdie/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/abdie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-content-migration/21-RESEARCH.md
@.planning/phases/21-content-migration/21-CONTEXT.md
@.planning/phases/21-content-migration/21-01-SUMMARY.md
@lib/articles-s3.js
@lib/articles.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Execute live migration and run automated verification</name>
  <files>scripts/migrate-blog-to-s3.js</files>
  <action>
**Pre-flight check:**
1. Confirm S3_IMAGES_BUCKET_NAME and S3_ARTICLES_BUCKET_NAME env vars are set (the user must have these configured)
2. Run `node scripts/migrate-blog-to-s3.js --dry-run` one final time to confirm everything looks good
3. If dry-run succeeds, proceed to live migration

**Execute live migration:**
```bash
node scripts/migrate-blog-to-s3.js --live
```

Wait for completion. The script should:
- Upload all 215 images to S3_IMAGES_BUCKET_NAME
- Upload all 77 articles as JSON to S3_ARTICLES_BUCKET_NAME
- Build and upload articles/index.json
- Create migration log file

**Automated Verification (all four methods from CONTEXT.md):**

**Method 1 — Count check:**
- Verify script output shows 77 articles migrated and 215 images uploaded
- Parse the migration log JSONL file to count entries with `status: "success"` for both images and articles
- If counts don't match (77 articles, 215 images), report discrepancy

**Method 2 — Sample rendering:**
After migration completes, test that the blog can read from S3 by running:
```bash
npm run build
```
This exercises `generateStaticParams()` which calls `listArticles()` from S3, and `generateMetadata()` which calls `getArticle()` for each article. If the build succeeds, it means:
- The article index is readable from S3
- Individual articles are accessible
- MDX content compiles successfully with next-mdx-remote

If the build fails, report the error and stop (do NOT proceed to archival).

**Method 3 — Spot-check list output:**
The migration script already outputs the spot-check list. Capture it from the output for the checkpoint in Task 2.

**Method 4 — Full build test:**
Already covered by the `npm run build` above.

If any automated verification fails, STOP and report the issue. Do NOT proceed to Task 2 or Task 3.
  </action>
  <verify>
1. Migration script completes with "Migration Complete!" message
2. Log file shows 77 articles with status "success" and 215 images with status "success"
3. `npm run build` completes successfully
4. Build output shows static pages generated for /blog routes
  </verify>
  <done>
All 77 articles and 215 images are in S3. The build passes, confirming articles are readable and MDX compiles correctly. Migration log file confirms all operations succeeded.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: User spot-checks migrated posts</name>
  <what-built>
All 77 blog posts and 215 images have been migrated from the filesystem to S3. The build passes and static pages generate successfully. The blog is now reading content from S3.
  </what-built>
  <how-to-verify>
Start the dev server and manually check these 5 posts:

```bash
npm run dev
```

1. **Oldest post** — Visit http://localhost:3000/blog/2017/04/13/PlanetaDosSoles04132017
   - Confirm: Title renders, images load from S3, content is readable

2. **Newest post** — Visit http://localhost:3000/blog/2025/08/21/siete-cometas-tendran-un-leve-acercamiento-a-la-tierra
   - Confirm: Title renders, images load from S3 (some have spaces in filenames), content is readable

3. **Video embeds** — Visit http://localhost:3000/blog/2024/11/26/Starship_Electrophonic_Sound
   - Confirm: Images load, ResponsiveReactPlayer video embeds render (YouTube iframes appear)

4. **Complex layout** — Visit http://localhost:3000/blog/telescopios
   - Confirm: Multiple inline images load from S3, complex JSX/HTML layout renders correctly

5. **Many images** — Visit http://localhost:3000/blog/2019/10/07/Saturn20
   - Confirm: Multiple images and GIF load, content renders correctly

Also check:
- Visit http://localhost:3000/blog — Confirm the blog index shows articles with correct titles, dates, and summaries
- Visit http://localhost:3000/tags — Confirm tag cloud shows tags with correct counts
- Visit http://localhost:3000/feed.xml — Confirm RSS feed contains articles
  </how-to-verify>
  <resume-signal>Type "approved" if all posts render correctly, or describe any issues found.</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Archive original files to backup git branch</name>
  <files></files>
  <action>
After user approval, archive the original MDX blog files and blog images to a backup branch. This preserves the originals for rollback if ever needed, without cluttering the main branch.

**Create backup branch:**
```bash
git checkout -b backup/pre-s3-migration
git checkout feature/article-manager
```

The backup branch already contains all the original files since we haven't deleted anything yet. The branch itself serves as the archive.

Actually, the cleaner approach: The current branch already has all the original files in its git history. The "backup branch" should be a snapshot of the current state BEFORE we remove the files. So:

1. Create the backup branch from the current HEAD (before deletion):
   ```bash
   git branch backup/pre-s3-migration
   ```
   This creates the branch pointing at the current commit without switching to it.

2. Now, on the current branch (feature/article-manager), remove the original files:
   - Delete `data/blog/` directory (all MDX/MD posts)
   - Delete `public/static/images/blog/` directory (all blog images)

3. Commit the deletion:
   ```bash
   git add data/blog/ public/static/images/blog/
   git commit -m "chore(21-02): archive original blog content to S3 (originals on backup/pre-s3-migration branch)"
   ```

**Important:** Do NOT delete `data/authors/` — author files stay on disk per project decision (low volume, rarely changes).

**Important:** Do NOT delete `public/static/images/` entirely — only delete the `blog/` subdirectory. Other images (telescope_color.png, canada/, ocean.jpg, etc.) are not blog images and must remain.

**Verify after deletion:**
- `ls data/blog/` should fail (directory removed)
- `ls public/static/images/blog/` should fail (directory removed)
- `ls data/authors/` should still list author files
- `ls public/static/images/` should still exist with non-blog images
- `git branch` should show `backup/pre-s3-migration` exists
- `npm run build` should still pass (blog reads from S3, not filesystem)
  </action>
  <verify>
1. `git branch --list backup/pre-s3-migration` shows the backup branch exists
2. `data/blog/` directory no longer exists on the current branch
3. `public/static/images/blog/` directory no longer exists on the current branch
4. `data/authors/` still exists with author files
5. `npm run build` passes (blog content comes from S3)
  </verify>
  <done>
Original MDX files and blog images are archived on `backup/pre-s3-migration` branch. The current branch has them removed. The blog still works because all content now comes from S3. A full build passes confirming no filesystem dependencies remain.
  </done>
</task>

</tasks>

<verification>
1. Migration log file confirms 77 articles and 215 images migrated successfully
2. `npm run build` passes after migration (articles readable from S3)
3. User spot-checks 5 specific posts and confirms correct rendering
4. Blog index, tag pages, and RSS feed all show content
5. Backup branch exists with original files
6. Original files removed from working branch
7. Final `npm run build` passes after file removal
</verification>

<success_criteria>
- Every existing MDX post has a corresponding articles/{slug}.json in S3
- All blog images are in S3 and image references point to S3 URLs
- The public blog renders all migrated posts correctly (user confirmed)
- Original files archived to backup/pre-s3-migration branch
- The blog builds and runs entirely from S3 data with no filesystem blog content
</success_criteria>

<output>
After completion, create `.planning/phases/21-content-migration/21-02-SUMMARY.md`
</output>
